"""
Script para detectar y agrupar temas relevantes en las noticias usando Gemini AI.
Versi√≥n MVP: Agrupaci√≥n b√°sica y generaci√≥n de res√∫menes por tema.
"""

import json
import os
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict
import logging
import re

try:
    from google import genai
except ImportError:
    logging.error("Error: google-genai no est√° instalado.")
    logging.error("Inst√°lalo con: pip install google-genai")
    exit(1)

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Rutas
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
TEMAS_DIR = DATA_DIR / "temas"
FRONTEND_DIR = BASE_DIR / "frontend" / "data"
# Intentar usar noticias con contenido completo desde data/, fallback a frontend/data/
NOTICIAS_DIR = DATA_DIR

# L√≠mites
MAX_NOTICIAS_ANALIZAR = 150  # M√°ximo de noticias para analizar
MAX_TEMAS_DETECTAR = 10      # M√°ximo de temas a detectar
MIN_FUENTES_POR_TEMA = 2     # M√≠nimo de fuentes diferentes por tema


def obtener_api_key() -> str:
    """
    Obtiene la API key de Gemini de la variable de entorno o archivo .env.
    
    Returns:
        API key de Gemini
    """
    api_key = os.getenv("GEMINI_API_KEY")
    
    if api_key:
        api_key = api_key.strip().strip('"').strip("'")
    
    if not api_key or api_key == "":
        env_file = BASE_DIR / ".env"
        if env_file.exists():
            with open(env_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.startswith("GEMINI_API_KEY="):
                        api_key = line.split("=", 1)[1].strip().strip('"').strip("'")
                        break
    
    if not api_key or api_key == "":
        raise ValueError("GEMINI_API_KEY no est√° configurada")
    
    return api_key


def cargar_noticias_del_dia() -> Dict:
    """
    Intenta cargar noticias con contenido completo desde data/.
    Si no existe, usa las del frontend/ (sin contenido completo).
    
    Returns:
        Diccionario con los datos del JSON
    """
    # Intentar cargar archivo con contenido desde data/
    archivos_contenido = list(DATA_DIR.glob("noticias_contenido_*.json"))
    
    if archivos_contenido:
        archivo = max(archivos_contenido, key=lambda p: p.stat().st_mtime)
        logging.info(f"‚úì Usando noticias CON contenido completo: {archivo.name}")
    else:
        # Fallback a archivo normal desde frontend/data/
        archivos = list(FRONTEND_DIR.glob("noticias_[0-9]*.json"))
        if not archivos:
            raise FileNotFoundError(f"No se encontr√≥ ning√∫n archivo de noticias")
        archivo = max(archivos, key=lambda p: p.stat().st_mtime)
        logging.warning(f"‚ö†Ô∏è  Usando noticias SIN contenido completo: {archivo.name}")
    
    with open(archivo, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    return data


def inicializar_historico() -> Dict:
    """
    Crea estructura vac√≠a de hist√≥rico.
    
    Returns:
        Diccionario vac√≠o de hist√≥rico
    """
    return {
        'temas': {},
        'ultima_actualizacion': datetime.now().isoformat(),
        'total_temas_activos': 0,
        'total_temas_inactivos': 0,
        'total_temas_historicos': 0
    }


def cargar_historico() -> Dict:
    """
    Carga el archivo hist√≥rico de temas.
    Si no existe, retorna estructura vac√≠a.
    
    Returns:
        Diccionario con hist√≥rico de temas
    """
    archivo_historico = TEMAS_DIR / "historico_temas.json"
    
    if not archivo_historico.exists():
        logging.info("üìÇ No existe hist√≥rico previo, creando nuevo")
        return inicializar_historico()
    
    try:
        with open(archivo_historico, "r", encoding="utf-8") as f:
            historico = json.load(f)
        logging.info(f"‚úì Hist√≥rico cargado: {len(historico.get('temas', {}))} temas registrados")
        return historico
    except Exception as e:
        logging.error(f"Error al cargar hist√≥rico: {str(e)}")
        return inicializar_historico()


def guardar_historico(historico: Dict):
    """
    Guarda el hist√≥rico actualizado en data/temas/ y frontend/data/.
    
    Args:
        historico: Diccionario completo del hist√≥rico
    """
    # Actualizar timestamp
    historico['ultima_actualizacion'] = datetime.now().isoformat()
    
    # Recalcular contadores
    temas_activos = sum(1 for t in historico['temas'].values() if t.get('estado') == 'activo')
    temas_inactivos = sum(1 for t in historico['temas'].values() if t.get('estado') == 'inactivo')
    
    historico['total_temas_activos'] = temas_activos
    historico['total_temas_inactivos'] = temas_inactivos
    historico['total_temas_historicos'] = len(historico['temas'])
    
    # Guardar en data/temas/
    TEMAS_DIR.mkdir(parents=True, exist_ok=True)
    archivo_historico = TEMAS_DIR / "historico_temas.json"
    
    with open(archivo_historico, "w", encoding="utf-8") as f:
        json.dump(historico, f, ensure_ascii=False, indent=2)
    
    logging.info(f"‚úì Hist√≥rico guardado: {archivo_historico.name}")
    
    # NOTA: NO se copia a frontend/data/ porque el frontend no lo usa
    # El hist√≥rico solo se mantiene en data/temas/ para tracking interno


def calcular_similitud_simple(texto1: str, texto2: str) -> float:
    """
    Calcula similitud b√°sica entre dos textos (0.0 a 1.0).
    Usa comparaci√≥n de palabras en com√∫n (Jaccard similarity).
    
    Args:
        texto1: Primer texto normalizado
        texto2: Segundo texto normalizado
        
    Returns:
        Valor de 0.0 (nada similar) a 1.0 (id√©ntico)
    """
    palabras1 = set(texto1.split())
    palabras2 = set(texto2.split())
    
    if not palabras1 or not palabras2:
        return 0.0
    
    comunes = palabras1.intersection(palabras2)
    union = palabras1.union(palabras2)
    
    return len(comunes) / len(union) if union else 0.0


def encontrar_tema_existente(nombre_tema: str, nombre_normalizado: str, historico: Dict, umbral: float = 0.75) -> str:
    """
    Busca si el tema ya existe en el hist√≥rico.
    
    Args:
        nombre_tema: Nombre original del tema
        nombre_normalizado: Nombre normalizado del tema
        historico: Diccionario con hist√≥rico de temas
        umbral: Umbral de similitud para match (default 0.75 = 75%)
        
    Returns:
        tema_id si existe, None si es nuevo
    """
    for tema_id, datos in historico.get('temas', {}).items():
        # 1. Match exacto por nombre normalizado
        if datos.get('tema_normalizado') == nombre_normalizado:
            logging.info(f"  üîó Match exacto: '{nombre_tema}' = '{datos['tema']}'")
            return tema_id
        
        # 2. Match por alias
        if nombre_normalizado in datos.get('alias', []):
            logging.info(f"  üîó Match por alias: '{nombre_tema}' = '{datos['tema']}'")
            return tema_id
        
        # 3. Match por similitud (palabras en com√∫n)
        similitud = calcular_similitud_simple(nombre_normalizado, datos.get('tema_normalizado', ''))
        if similitud >= umbral:
            logging.info(f"  üîó Match por similitud ({int(similitud*100)}%): '{nombre_tema}' ‚âà '{datos['tema']}'")
            return tema_id
    
    # No se encontr√≥ match
    logging.info(f"  ‚≠ê Tema NUEVO: '{nombre_tema}'")
    return None


def seleccionar_noticias_para_analisis(noticias: List[Dict], max_noticias: int = MAX_NOTICIAS_ANALIZAR) -> List[Dict]:
    """
    Selecciona las noticias m√°s relevantes para an√°lisis de temas.
    Prioriza noticias de categor√≠as principales y m√°s recientes.
    
    Args:
        noticias: Lista completa de noticias
        max_noticias: M√°ximo de noticias a incluir
        
    Returns:
        Lista filtrada de noticias
    """
    # Filtrar por categor√≠as principales
    categorias_principales = ['internacional', 'politica', 'economia', 'sociedad']
    
    noticias_filtradas = [
        n for n in noticias 
        if n.get('categoria_url', '').lower() in categorias_principales
    ]
    
    # Si no hay suficientes, usar todas
    if len(noticias_filtradas) < max_noticias:
        noticias_filtradas = noticias
    
    # Ordenar por fecha (m√°s recientes primero) y tomar las primeras max_noticias
    noticias_ordenadas = sorted(
        noticias_filtradas,
        key=lambda n: n.get('fecha_local', ''),
        reverse=True
    )
    
    return noticias_ordenadas[:max_noticias]


def calcular_tendencia(apariciones: List[Dict]) -> str:
    """
    Calcula la tendencia del tema bas√°ndose en las √∫ltimas apariciones.
    
    Args:
        apariciones: Lista de apariciones del tema
        
    Returns:
        "nuevo", "creciente", "estable", "decreciente"
    """
    if len(apariciones) < 2:
        return "nuevo"
    
    # Comparar √∫ltimas dos apariciones
    ultimas_dos = apariciones[-2:]
    cantidad_anterior = ultimas_dos[0].get('cantidad_noticias', 0)
    cantidad_actual = ultimas_dos[1].get('cantidad_noticias', 0)
    
    diff = cantidad_actual - cantidad_anterior
    
    if diff > 0:
        return "creciente"
    elif diff < 0:
        return "decreciente"
    else:
        return "estable"


def calcular_metricas(apariciones: List[Dict]) -> Dict:
    """
    Calcula m√©tricas estad√≠sticas del tema.
    
    Args:
        apariciones: Lista de apariciones del tema
        
    Returns:
        Diccionario con m√©tricas
    """
    if not apariciones:
        return {
            'pico_noticias': 0,
            'minimo_noticias': 0,
            'promedio_noticias_dia': 0.0,
            'fuentes_unicas': []
        }
    
    cantidades = [a.get('cantidad_noticias', 0) for a in apariciones]
    todas_fuentes = set()
    for a in apariciones:
        todas_fuentes.update(a.get('fuentes', []))
    
    return {
        'pico_noticias': max(cantidades) if cantidades else 0,
        'minimo_noticias': min(cantidades) if cantidades else 0,
        'promedio_noticias_dia': round(sum(cantidades) / len(cantidades), 1) if cantidades else 0.0,
        'fuentes_unicas': sorted(list(todas_fuentes))
    }


def actualizar_estado_temas(historico: Dict, fecha_actual: str):
    """
    Actualiza el estado de todos los temas en el hist√≥rico.
    Marca como inactivos los que no aparecieron en los √∫ltimos 3 d√≠as.
    
    Args:
        historico: Diccionario del hist√≥rico
        fecha_actual: Fecha de hoy en formato YYYY-MM-DD
    """
    from datetime import datetime, timedelta
    
    fecha_hoy = datetime.strptime(fecha_actual, "%Y-%m-%d")
    
    for tema_id, datos in historico.get('temas', {}).items():
        fecha_ultima = datos.get('fecha_ultima_aparicion', fecha_actual)
        fecha_ultima_dt = datetime.strptime(fecha_ultima, "%Y-%m-%d")
        
        dias_sin_aparecer = (fecha_hoy - fecha_ultima_dt).days
        
        # Marcar como inactivo si no apareci√≥ por 3+ d√≠as
        if dias_sin_aparecer >= 3 and datos.get('estado') == 'activo':
            datos['estado'] = 'inactivo'
            datos['dias_inactivo'] = dias_sin_aparecer
            logging.info(f"  ‚è∏Ô∏è  Tema inactivo: '{datos['tema']}' ({dias_sin_aparecer} d√≠as sin aparecer)")
        
        # Si reaparece despu√©s de estar inactivo, reactivar
        elif dias_sin_aparecer == 0 and datos.get('estado') == 'inactivo':
            datos['estado'] = 'reactivado'
            datos['dias_inactivo'] = 0
            logging.info(f"  ‚ñ∂Ô∏è  Tema reactivado: '{datos['tema']}'")


def limpiar_apariciones_antiguas(historico: Dict, max_dias: int = 30):
    """
    Limita el hist√≥rico a los √∫ltimos X d√≠as para cada tema.
    
    Args:
        historico: Diccionario del hist√≥rico
        max_dias: M√°ximo de d√≠as a mantener
    """
    for tema_id, datos in historico.get('temas', {}).items():
        apariciones = datos.get('apariciones', [])
        
        if len(apariciones) > max_dias:
            # Mantener solo las √∫ltimas max_dias apariciones
            antes = len(apariciones)
            datos['apariciones'] = apariciones[-max_dias:]
            logging.info(f"  üóëÔ∏è  Limpiadas apariciones antiguas de '{datos['tema']}': {antes} ‚Üí {len(datos['apariciones'])}")


def limpiar_html(texto: str) -> str:
    """Limpia HTML del texto."""
    if not texto:
        return ""
    texto = re.sub(r'<[^>]+>', '', texto)
    texto = re.sub(r'\s+', ' ', texto)
    return texto.strip()


def crear_prompt_agrupacion(noticias: List[Dict]) -> str:
    """
    Crea el prompt para que Gemini agrupe las noticias por tema.
    
    Args:
        noticias: Lista de noticias a agrupar
        
    Returns:
        Prompt completo
    """
    # Construir lista de titulares numerados
    lista_titulares = ""
    for idx, noticia in enumerate(noticias, 1):
        titulo = noticia.get('titulo', 'Sin t√≠tulo')
        fuente = noticia.get('fuente', '')
        categoria = noticia.get('categoria_url', '')
        lista_titulares += f"{idx}. {titulo} [{fuente} - {categoria}]\n"
    
    prompt = f"""Analiz√° los siguientes {len(noticias)} titulares de noticias argentinas y agrupalos por tema ESPEC√çFICO.

TITULARES:
{lista_titulares}

CRITERIOS DE AGRUPACI√ìN:
1. Cre√° nombres de temas ESPEC√çFICOS Y CONCRETOS, no gen√©ricos
2. Identific√° el evento, conflicto o situaci√≥n PARTICULAR
3. Ejemplos de nombres CORRECTOS:
   ‚úì "Operativo policial en R√≠o de Janeiro contra narcotr√°fico"
   ‚úì "Negociaci√≥n salarial docente 2025"
   ‚úì "Causa Cuadernos: nuevos testimonios"
   ‚úì "D√≥lar paralelo: escalada noviembre"
   ‚úì "Hurac√°n Melissa: paso por el Caribe"
4. Ejemplos de nombres INCORRECTOS (demasiado gen√©ricos):
   ‚úó "Geopol√≠tica y conflictos globales"
   ‚úó "Econom√≠a argentina"
   ‚úó "Pol√≠tica nacional"
   ‚úó "Noticias internacionales"
5. Cada grupo debe tener al menos 2 titulares relacionados DE DIFERENTES FUENTES
6. Un titular puede pertenecer solo a UN tema
7. M√°ximo {MAX_TEMAS_DETECTAR} temas
8. Si un titular no encaja claramente en ning√∫n grupo, no lo fuerces
9. Nombres de 3-8 palabras
10. Prioriz√° temas con mayor cobertura (m√°s fuentes = m√°s relevante)

FORMATO DE RESPUESTA (JSON √∫nicamente, sin texto adicional):
{{
  "temas": [
    {{
      "tema": "Nombre espec√≠fico del tema o evento",
      "indices_titulares": [1, 3, 5]
    }}
  ]
}}

Respond√© SOLO con el JSON:"""
    
    return prompt


def crear_prompt_resumen(tema: str, noticias_relacionadas: List[Dict]) -> str:
    """
    Crea el prompt para generar el resumen de un tema NUEVO.
    Usa contenido completo si est√° disponible, sino usa el resumen RSS.
    
    Args:
        tema: Nombre del tema
        noticias_relacionadas: Lista de diccionarios con datos completos de las noticias
        
    Returns:
        Prompt completo
    """
    noticias_texto = ""
    
    for idx, noticia in enumerate(noticias_relacionadas, 1):
        titulo = noticia.get('titulo', 'Sin t√≠tulo')
        noticias_texto += f"{idx}. {titulo}\n"
        
        # Usar contenido completo si existe, sino usar resumen RSS
        if noticia.get('contenido_extraido') and noticia.get('contenido_completo'):
            # Limitar a primeras 500 palabras para no saturar prompt
            contenido = ' '.join(noticia['contenido_completo'].split()[:500])
            palabras = noticia.get('palabras_contenido', 0)
            noticias_texto += f"   [Contenido completo - {palabras} palabras]:\n"
            noticias_texto += f"   {contenido}...\n\n"
        else:
            # Fallback a resumen RSS
            resumen = noticia.get('resumen', 'Sin descripci√≥n')
            resumen_limpio = limpiar_html(resumen) if resumen else ""
            if resumen_limpio:
                noticias_texto += f"   [Resumen RSS]: {resumen_limpio[:200]}...\n\n"
            else:
                noticias_texto += f"   [Sin contenido disponible]\n\n"
    
    prompt = f"""Cre√° un resumen SIMPLE y DID√ÅCTICO sobre este tema, f√°cil de leer y bien estructurado.

TEMA: {tema}

NOTICIAS RELACIONADAS:
{noticias_texto}

FORMATO REQUERIDO:
Us√° esta estructura SIEMPRE (NO uses p√°rrafos largos):

üìå **¬øQu√© est√° pasando?**
‚Ä¢ [En 2-3 oraciones cortas, explicar el hecho principal]
‚Ä¢ [Usar bullets, NO p√°rrafos]

üîç **Datos clave:**
‚Ä¢ [Cifra/fecha/nombre importante #1]
‚Ä¢ [Cifra/fecha/nombre importante #2]
‚Ä¢ [Cifra/fecha/nombre importante #3]
‚Ä¢ [Agregar m√°s si hay informaci√≥n relevante]

‚ö° **Desarrollo:**
‚Ä¢ [Punto importante #1 del contexto]
‚Ä¢ [Punto importante #2 de las causas o consecuencias]
‚Ä¢ [Punto importante #3 sobre actores involucrados]
‚Ä¢ [Continuar con m√°s bullets seg√∫n necesidad]

üí° **¬øPor qu√© importa?**
‚Ä¢ [Explicar en 1-2 bullets el impacto o relevancia]

REGLAS:
- Comenz√° DIRECTO sin saludos
- Us√° BULLETS (‚Ä¢), NO p√°rrafos largos
- M√°ximo 2-3 oraciones por bullet
- Inclu√≠ TODOS los datos concretos: cifras, fechas, nombres
- Entre 300-400 palabras
- Tono claro, profesional y neutral

Resumen:"""
    
    return prompt


def crear_prompt_resumen_recurrente(tema: str, resumen_anterior: str, noticias_nuevas: List[Dict]) -> str:
    """
    Crea el prompt para actualizar el resumen de un tema RECURRENTE.
    Integra el resumen anterior con las novedades de hoy.
    
    Args:
        tema: Nombre del tema
        resumen_anterior: Resumen del d√≠a anterior
        noticias_nuevas: Noticias nuevas de hoy relacionadas al tema
        
    Returns:
        Prompt completo
    """
    # Construir lista de noticias nuevas
    noticias_texto = ""
    for idx, noticia in enumerate(noticias_nuevas, 1):
        titulo = noticia.get('titulo', 'Sin t√≠tulo')
        noticias_texto += f"{idx}. {titulo}\n"
        
        # Usar contenido completo si est√° disponible
        if noticia.get('contenido_extraido') and noticia.get('contenido_completo'):
            contenido = ' '.join(noticia['contenido_completo'].split()[:500])
            palabras = noticia.get('palabras_contenido', 0)
            noticias_texto += f"   [Contenido completo - {palabras} palabras]:\n"
            noticias_texto += f"   {contenido}...\n\n"
        else:
            resumen = noticia.get('resumen', 'Sin descripci√≥n')
            resumen_limpio = limpiar_html(resumen) if resumen else ""
            if resumen_limpio:
                noticias_texto += f"   [Resumen RSS]: {resumen_limpio[:200]}...\n\n"
    
    prompt = f"""Actualiz√° el resumen de este tema con las novedades de hoy. Manten√© el formato SIMPLE y DID√ÅCTICO.

TEMA: {tema}

RESUMEN ANTERIOR:
{resumen_anterior}

NOTICIAS NUEVAS DE HOY:
{noticias_texto}

FORMATO REQUERIDO:
Manten√© la misma estructura (NO uses p√°rrafos largos):

üìå **¬øQu√© est√° pasando?**
‚Ä¢ [Actualizar con lo M√ÅS RECIENTE en 2-3 oraciones cortas]
‚Ä¢ [Mantener contexto si sigue siendo relevante]

üîç **Datos clave:**
‚Ä¢ [ACTUALIZAR cifras/fechas si cambiaron]
‚Ä¢ [Mantener datos relevantes del resumen anterior]
‚Ä¢ [Agregar nuevos datos importantes de hoy]

‚ö° **Desarrollo:**
‚Ä¢ [INTEGRAR novedades de hoy con el contexto previo]
‚Ä¢ [Conectar: ¬øc√≥mo evolucion√≥ la situaci√≥n?]
‚Ä¢ [Eliminar informaci√≥n obsoleta o menos relevante]
‚Ä¢ [Agregar puntos nuevos importantes]

üí° **¬øPor qu√© importa?**
‚Ä¢ [Actualizar el impacto o relevancia seg√∫n novedades]

REGLAS:
- Comenz√° DIRECTO sin saludos
- INTEGR√Å las novedades (no las agregues como anexo)
- Us√° BULLETS (‚Ä¢), NO p√°rrafos largos
- ACTUALIZ√Å cifras y datos obsoletos
- Elimin√° info que ya no es central
- Entre 300-400 palabras
- Tono claro, profesional y neutral

Resumen actualizado:"""
    
    return prompt


def llamar_gemini_con_retry(client, model: str, prompt: str, max_intentos: int = 3) -> str:
    """
    Llama a Gemini con reintentos autom√°ticos si el servicio est√° sobrecargado.
    
    Args:
        client: Cliente de Gemini
        model: Nombre del modelo
        prompt: Prompt a enviar
        max_intentos: N√∫mero m√°ximo de intentos
        
    Returns:
        Texto de la respuesta
    """
    
    for intento in range(max_intentos):
        try:
            response = client.models.generate_content(
                model=model,
                contents=[prompt]
            )
            return getattr(response, "text", "") or ""
            
        except Exception as e:
            error_msg = str(e)
            
            # Si es error 503 (sobrecarga) o 429 (rate limit), reintentar
            if "503" in error_msg or "429" in error_msg or "UNAVAILABLE" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                if intento < max_intentos - 1:
                    espera = (intento + 1) * 5  # 5, 10, 15 segundos (m√°s tiempo para rate limit)
                    logging.warning(f"Servicio sobrecargado. Reintentando en {espera}s... (intento {intento + 1}/{max_intentos})")
                    time.sleep(espera)
                    continue
                else:
                    logging.error(f"Servicio sobrecargado despu√©s de {max_intentos} intentos")
                    raise
            else:
                # Otro tipo de error, no reintentar
                raise
    
    raise Exception("No se pudo completar la solicitud despu√©s de m√∫ltiples intentos")


def agrupar_con_gemini(noticias: List[Dict], api_key: str) -> Dict:
    """
    Llama a Gemini para agrupar noticias por tema.
    
    Args:
        noticias: Lista de noticias
        api_key: API key de Gemini
        
    Returns:
        Diccionario con temas y sus √≠ndices
    """
    prompt = crear_prompt_agrupacion(noticias)
    
    try:
        client = genai.Client(api_key=api_key)
        
        # Usar gemini-2.5-flash
        texto_respuesta = llamar_gemini_con_retry(client, "gemini-2.5-flash", prompt)
        
        # Limpiar la respuesta para extraer solo el JSON
        if "```json" in texto_respuesta:
            texto_respuesta = texto_respuesta.split("```json")[1].split("```")[0]
        elif "```" in texto_respuesta:
            texto_respuesta = texto_respuesta.split("```")[1].split("```")[0]
        
        # Parsear JSON
        resultado = json.loads(texto_respuesta.strip())
        return resultado
        
    except json.JSONDecodeError as e:
        logging.error(f"Error al parsear JSON de Gemini: {str(e)}")
        logging.error(f"Respuesta recibida: {texto_respuesta[:500]}")
        raise
    except Exception as e:
        logging.error(f"Error al llamar a Gemini para agrupaci√≥n: {str(e)}")
        raise


def generar_resumen_tema(tema: str, noticias_relacionadas: List[Dict], api_key: str, resumen_anterior: str = None) -> str:
    """
    Genera o actualiza el resumen de un tema usando Gemini.
    
    Args:
        tema: Nombre del tema
        noticias_relacionadas: Lista de noticias completas relacionadas al tema
        api_key: API key de Gemini
        resumen_anterior: Si existe, integra las novedades. Si es None, genera desde cero
        
    Returns:
        Resumen generado o actualizado
    """
    # Decidir qu√© prompt usar seg√∫n si es tema nuevo o recurrente
    if resumen_anterior:
        logging.info(f"  üîÑ Integrando resumen recurrente...")
        prompt = crear_prompt_resumen_recurrente(tema, resumen_anterior, noticias_relacionadas)
    else:
        logging.info(f"  ‚ú® Generando resumen nuevo...")
        prompt = crear_prompt_resumen(tema, noticias_relacionadas)
    
    try:
        client = genai.Client(api_key=api_key)
        
        # Usar gemini-2.5-flash con retry
        texto_respuesta = llamar_gemini_con_retry(client, "gemini-2.5-flash", prompt)
        
        return texto_respuesta.strip()
        
    except Exception as e:
        logging.error(f"Error al generar resumen para '{tema}': {str(e)}")
        return f"No se pudo generar resumen debido a sobrecarga del servicio. Por favor, intente m√°s tarde."


def normalizar_nombre_tema(nombre: str) -> str:
    """
    Normaliza el nombre de un tema para comparaciones y generaci√≥n de IDs.
    
    Args:
        nombre: Nombre original del tema
        
    Returns:
        Nombre normalizado
    """
    # Min√∫sculas
    norm = nombre.lower()
    
    # Remover art√≠culos
    norm = re.sub(r'\b(el|la|los|las|un|una|unos|unas)\b', '', norm)
    
    # Remover acentos
    acentos = {'√°':'a', '√©':'e', '√≠':'i', '√≥':'o', '√∫':'u', '√±':'n'}
    for acento, sin_acento in acentos.items():
        norm = norm.replace(acento, sin_acento)
    
    # Limpiar espacios m√∫ltiples
    norm = re.sub(r'\s+', ' ', norm).strip()
    
    return norm


def generar_tema_id(nombre_normalizado: str, fecha: str) -> str:
    """
    Genera un ID √∫nico para el tema.
    
    Args:
        nombre_normalizado: Nombre normalizado del tema
        fecha: Fecha de detecci√≥n
        
    Returns:
        ID del tema
    """
    # Tomar primeras palabras del nombre normalizado
    palabras = nombre_normalizado.split()[:3]
    base = '_'.join(palabras)
    
    # Agregar fecha para unicidad
    fecha_corta = fecha.replace('-', '')
    
    return f"{base}_{fecha_corta}"


def main():
    """
    Funci√≥n principal que detecta y agrupa temas.
    """
    logging.info("=" * 70)
    logging.info("DETECCI√ìN Y AGRUPACI√ìN DE TEMAS CON IA")
    logging.info("=" * 70)
    
    # 1. Obtener API key
    try:
        api_key = obtener_api_key()
        logging.info("‚úì API key de Gemini configurada")
    except ValueError as e:
        logging.error(str(e))
        return
    
    # 2. Cargar noticias del d√≠a
    try:
        data = cargar_noticias_del_dia()
        noticias = data.get("noticias", [])
        fecha_consolidacion = data.get("fecha_consolidacion", datetime.now().strftime("%Y-%m-%d"))
        logging.info(f"‚úì Cargadas {len(noticias)} noticias del {fecha_consolidacion}")
    except Exception as e:
        logging.error(f"Error al cargar noticias: {str(e)}")
        return
    
    # 3. Cargar hist√≥rico de temas
    logging.info("\n" + "=" * 70)
    logging.info("CARGANDO HIST√ìRICO DE TEMAS")
    logging.info("=" * 70)
    historico = cargar_historico()
    
    # 4. Actualizar estados de temas existentes (marcar inactivos)
    actualizar_estado_temas(historico, fecha_consolidacion)
    
    # 5. Seleccionar noticias para an√°lisis
    noticias_seleccionadas = seleccionar_noticias_para_analisis(noticias)
    logging.info(f"Seleccionadas {len(noticias_seleccionadas)} noticias para an√°lisis de temas")
    
    # 6. Agrupar noticias por tema con Gemini
    logging.info("\n" + "=" * 70)
    logging.info("AGRUPACI√ìN POR TEMAS (Gemini AI)")
    logging.info("=" * 70)
    
    try:
        resultado_agrupacion = agrupar_con_gemini(noticias_seleccionadas, api_key)
        temas_detectados = resultado_agrupacion.get("temas", [])
        logging.info(f"‚úì Detectados {len(temas_detectados)} temas")
        
        for tema_data in temas_detectados:
            logging.info(f"  ‚Ä¢ {tema_data['tema']}: {len(tema_data['indices_titulares'])} noticias")
            
    except Exception as e:
        logging.error(f"Error en agrupaci√≥n: {str(e)}")
        logging.warning("No se pudieron detectar temas nuevos por sobrecarga de la API")
        logging.warning("Intent√° ejecutar 'python scripts/agrupar_temas.py' m√°s tarde")
        
        # Aunque falle la agrupaci√≥n, copiar √∫ltimo temas_latest si existe
        import shutil
        FRONTEND_DIR.mkdir(parents=True, exist_ok=True)
        
        # Copiar √∫ltimo temas_latest si existe (el hist√≥rico NO se copia)
        archivos_temas = sorted(TEMAS_DIR.glob("temas_*.json"))
        if archivos_temas:
            try:
                ultimo_temas = archivos_temas[-1]
                archivo_frontend_temas = FRONTEND_DIR / "temas_latest.json"
                shutil.copy2(ultimo_temas, archivo_frontend_temas)
                logging.info(f"‚úì √öltimo archivo de temas copiado a frontend: {ultimo_temas.name}")
            except Exception as copy_error:
                logging.error(f"Error al copiar temas: {str(copy_error)}")
        
        return
    
    # 5. Generar res√∫menes por cada tema
    logging.info("\n" + "=" * 70)
    logging.info("GENERACI√ìN DE RES√öMENES")
    logging.info("=" * 70)
    
    # Peque√±o delay despu√©s de la agrupaci√≥n para respetar rate limit
    logging.info("‚è≥ Esperando 3s antes de comenzar generaci√≥n de res√∫menes...")
    time.sleep(3)
    
    temas_completos = []
    temas_pendientes = list(enumerate(temas_detectados, 1))  # (idx, tema_data)
    max_ciclos_reintento = 2
    ciclo = 0
    
    while temas_pendientes and ciclo <= max_ciclos_reintento:
        if ciclo > 0:
            logging.info("\n" + "=" * 70)
            logging.info(f"REINTENTANDO TEMAS FALLIDOS (Ciclo {ciclo})")
            logging.info("=" * 70)
            logging.info(f"‚è≥ Esperando 30s para resetear rate limit...")
            time.sleep(30)
        
        temas_para_siguiente_ciclo = []
        
        for idx_tema, tema_data in temas_pendientes:
            nombre_tema = tema_data['tema']
            indices = tema_data['indices_titulares']
            
            logging.info(f"\nProcesando tema {idx_tema}/{len(temas_detectados)}: {nombre_tema}")
            
            # Extraer noticias completas relacionadas (para usar contenido completo si est√° disponible)
            noticias_completas_relacionadas = []
            noticias_para_guardar = []
            fuentes_unicas = set()
            
            for idx in indices:
                if 0 <= idx - 1 < len(noticias_seleccionadas):  # indices son 1-based
                    noticia = noticias_seleccionadas[idx - 1]
                    fuente = noticia.get('fuente', '')
                    if fuente:
                        fuentes_unicas.add(fuente)
                    
                    # Guardar noticia completa para generar resumen (incluye contenido_completo si existe)
                    noticias_completas_relacionadas.append(noticia)
                    
                    # Guardar solo campos necesarios para el JSON final
                    noticias_para_guardar.append({
                        'titulo': noticia.get('titulo', ''),
                        'link': noticia.get('link', ''),
                        'fuente': fuente,
                        'fecha': noticia.get('fecha_local', '')
                    })
            
            # Validar que tenga al menos MIN_FUENTES_POR_TEMA fuentes diferentes
            if len(fuentes_unicas) < MIN_FUENTES_POR_TEMA:
                logging.warning(f"‚ö†Ô∏è  Tema '{nombre_tema}' tiene solo {len(fuentes_unicas)} fuente(s). Requiere m√≠nimo {MIN_FUENTES_POR_TEMA}. Saltando...")
                continue
            
            # Normalizar nombre del tema
            nombre_normalizado = normalizar_nombre_tema(nombre_tema)
            
            # Buscar si el tema ya existe en el hist√≥rico
            tema_id_existente = encontrar_tema_existente(nombre_tema, nombre_normalizado, historico)
            
            # Generar resumen con Gemini (nuevo o integrado seg√∫n si existe)
            resumen_exitoso = False
            resumen_anterior = None
            es_tema_nuevo = True
            es_tema_recurrente = False
            dias_activo = 1
            
            if tema_id_existente:
                # Tema RECURRENTE: integrar con resumen anterior
                tema_historico = historico['temas'][tema_id_existente]
                resumen_anterior = tema_historico.get('resumen_actual', '')
                es_tema_nuevo = False
                es_tema_recurrente = True
                tema_id = tema_id_existente
            else:
                # Tema NUEVO: generar ID nuevo
                tema_id = generar_tema_id(nombre_normalizado, fecha_consolidacion)
            
            try:
                resumen = generar_resumen_tema(
                    nombre_tema, 
                    noticias_completas_relacionadas, 
                    api_key,
                    resumen_anterior=resumen_anterior
                )
                
                # Verificar que el resumen no sea un mensaje de error
                if not resumen.startswith("No se pudo generar resumen") and not resumen.startswith("Error al generar"):
                    logging.info(f"‚úì Resumen generado ({len(resumen)} caracteres)")
                    
                    # Determinar categor√≠a principal (la m√°s frecuente, usando categoria_url)
                    categorias = []
                    for idx in indices:
                        if 0 <= idx - 1 < len(noticias_seleccionadas):
                            cat = noticias_seleccionadas[idx - 1].get('categoria_url', 'otros')
                            if cat and cat.strip():
                                categorias.append(cat.lower().strip())
                            else:
                                categorias.append('otros')
                    
                    categoria_principal = max(set(categorias), key=categorias.count) if categorias else 'otros'
                    fuentes_lista = list(set([n['fuente'] for n in noticias_para_guardar if n.get('fuente')]))
                    
                    # ACTUALIZAR O CREAR ENTRADA EN HIST√ìRICO
                    if es_tema_recurrente:
                        # Actualizar tema existente en hist√≥rico
                        tema_historico = historico['temas'][tema_id]
                        tema_historico['fecha_ultima_aparicion'] = fecha_consolidacion
                        tema_historico['dias_activo'] = tema_historico.get('dias_activo', 0) + 1
                        tema_historico['resumen_actual'] = resumen
                        tema_historico['categoria_principal'] = categoria_principal
                        tema_historico['estado'] = 'activo'
                        
                        # Agregar aparici√≥n de hoy
                        tema_historico['apariciones'].append({
                            'fecha': fecha_consolidacion,
                            'cantidad_noticias': len(noticias_para_guardar),
                            'resumen': resumen,
                            'fuentes': fuentes_lista,
                            'categoria_principal': categoria_principal
                        })
                        
                        # Calcular tendencia y m√©tricas
                        tema_historico['tendencia'] = calcular_tendencia(tema_historico['apariciones'])
                        tema_historico['metricas'] = calcular_metricas(tema_historico['apariciones'])
                        tema_historico['total_noticias_acumuladas'] = sum(a.get('cantidad_noticias', 0) for a in tema_historico['apariciones'])
                        
                        dias_activo = tema_historico['dias_activo']
                        
                    else:
                        # Crear nuevo tema en hist√≥rico
                        historico['temas'][tema_id] = {
                            'tema_id': tema_id,
                            'tema': nombre_tema,
                            'tema_normalizado': nombre_normalizado,
                            'alias': [],
                            'fecha_primer_deteccion': fecha_consolidacion,
                            'fecha_ultima_aparicion': fecha_consolidacion,
                            'dias_activo': 1,
                            'dias_consecutivos': 1,
                            'dias_inactivo': 0,
                            'apariciones': [{
                                'fecha': fecha_consolidacion,
                                'cantidad_noticias': len(noticias_para_guardar),
                                'resumen': resumen,
                                'fuentes': fuentes_lista,
                                'categoria_principal': categoria_principal
                            }],
                            'resumen_actual': resumen,
                            'categoria_principal': categoria_principal,
                            'total_noticias_acumuladas': len(noticias_para_guardar),
                            'estado': 'activo',
                            'tendencia': 'nuevo',
                            'metricas': calcular_metricas([{
                                'cantidad_noticias': len(noticias_para_guardar),
                                'fuentes': fuentes_lista
                            }])
                        }
                    
                    # Construir objeto de tema para el JSON del d√≠a
                    tema_completo = {
                        'tema_id': tema_id,
                        'tema': nombre_tema,
                        'tema_normalizado': nombre_normalizado,
                        'resumen': resumen,
                        'cantidad_noticias': len(noticias_para_guardar),
                        'noticias': noticias_para_guardar,
                        'categoria_principal': categoria_principal,
                        'fecha_deteccion': fecha_consolidacion,
                        'fuentes': fuentes_lista,
                        'es_tema_nuevo': es_tema_nuevo,
                        'es_tema_recurrente': es_tema_recurrente,
                        'dias_activo': dias_activo,
                        'tendencia': historico['temas'][tema_id].get('tendencia', 'nuevo')
                    }
                    
                    temas_completos.append(tema_completo)
                    resumen_exitoso = True
                else:
                    # Resumen fall√≥, agregar a pendientes para reintentar
                    logging.warning(f"‚ö†Ô∏è  Resumen fall√≥, se reintentar√° despu√©s")
                    temas_para_siguiente_ciclo.append((idx_tema, tema_data))
                
            except Exception as e:
                logging.error(f"‚úó Error al generar resumen: {str(e)}")
                temas_para_siguiente_ciclo.append((idx_tema, tema_data))
            
            # Delay entre res√∫menes para respetar rate limit de Gemini (15 RPM)
            # Solo si fue exitoso y no es el √∫ltimo tema de este ciclo
            if resumen_exitoso and (idx_tema, tema_data) != temas_pendientes[-1]:
                delay = 5  # 5 segundos entre cada resumen
                logging.info(f"‚è≥ Esperando {delay}s antes del siguiente tema (rate limit)...")
                time.sleep(delay)
        
        # Actualizar pendientes para el pr√≥ximo ciclo
        temas_pendientes = temas_para_siguiente_ciclo
        ciclo += 1
    
    # Resumen de procesamiento
    if temas_pendientes:
        logging.warning("\n" + "=" * 70)
        logging.warning(f"‚ö†Ô∏è  {len(temas_pendientes)} TEMAS NO PUDIERON PROCESARSE")
        logging.warning("=" * 70)
        for idx, tema_data in temas_pendientes:
            logging.warning(f"  ‚Ä¢ {tema_data['tema']}")
        logging.warning("Intenta ejecutar el script nuevamente m√°s tarde cuando la API est√© menos cargada.")
        logging.warning("=" * 70)
    
    # NUEVO: Limpiar apariciones antiguas del hist√≥rico (mantener √∫ltimas 30)
    limpiar_apariciones_antiguas(historico, max_dias=30)
    
    # NUEVO: Guardar hist√≥rico actualizado
    logging.info("\n" + "=" * 70)
    logging.info("ACTUALIZANDO HIST√ìRICO")
    logging.info("=" * 70)
    guardar_historico(historico)
    
    # 6. Guardar resultados del d√≠a
    logging.info("\n" + "=" * 70)
    logging.info("GUARDANDO RESULTADOS")
    logging.info("=" * 70)
    
    # Crear directorio de temas si no existe
    TEMAS_DIR.mkdir(parents=True, exist_ok=True)
    
    # Estructura final
    resultado_final = {
        'fecha': fecha_consolidacion,
        'fecha_generacion': datetime.now().isoformat(),
        'total_temas': len(temas_completos),
        'temas': temas_completos
    }
    
    # Guardar en data/temas/
    nombre_archivo = f"temas_{fecha_consolidacion}.json"
    archivo_salida = TEMAS_DIR / nombre_archivo
    
    try:
        with open(archivo_salida, "w", encoding="utf-8") as f:
            json.dump(resultado_final, f, ensure_ascii=False, indent=2)
        
        logging.info(f"‚úì Archivo guardado: {archivo_salida}")
        
        # Copiar tambi√©n a frontend/data/ como "latest"
        try:
            FRONTEND_DIR.mkdir(parents=True, exist_ok=True)
            import shutil
            
            archivo_frontend = FRONTEND_DIR / "temas_latest.json"
            shutil.copy2(archivo_salida, archivo_frontend)
            logging.info(f"‚úì Copiado a frontend: {archivo_frontend}")
            
        except Exception as e:
            logging.error(f"Error al copiar a frontend: {str(e)}")
        
        # Mostrar resumen
        logging.info("\n" + "=" * 70)
        logging.info("RESUMEN DE TEMAS DETECTADOS")
        logging.info("=" * 70)
        logging.info(f"Total detectados: {len(temas_detectados)}")
        logging.info(f"Procesados exitosamente: {len(temas_completos)}")
        logging.info(f"Fallidos: {len(temas_detectados) - len(temas_completos)}")
        
        # Estad√≠sticas de nuevos vs recurrentes
        temas_nuevos = sum(1 for t in temas_completos if t.get('es_tema_nuevo'))
        temas_recurrentes = sum(1 for t in temas_completos if t.get('es_tema_recurrente'))
        logging.info(f"Temas NUEVOS: {temas_nuevos} ‚≠ê")
        logging.info(f"Temas RECURRENTES: {temas_recurrentes} üîÑ")
        logging.info("=" * 70)
        
        for tema in temas_completos:
            tipo = "‚≠ê NUEVO" if tema.get('es_tema_nuevo') else f"üîÑ {tema.get('dias_activo', 1)} d√≠as"
            tendencia_emoji = {
                'nuevo': '‚ú®',
                'creciente': '‚Üë',
                'estable': '‚Üí',
                'decreciente': '‚Üì'
            }.get(tema.get('tendencia', 'nuevo'), '')
            
            logging.info(f"‚Ä¢ {tema['tema']} [{tipo}] {tendencia_emoji}")
            logging.info(f"  - {tema['cantidad_noticias']} noticias")
            logging.info(f"  - Categor√≠a: {tema['categoria_principal']}")
            logging.info(f"  - Fuentes: {', '.join(tema['fuentes'][:3])}")
        
        logging.info("=" * 70)
        
    except Exception as e:
        logging.error(f"Error al guardar resultados: {str(e)}")


if __name__ == "__main__":
    main()

