# ğŸ“° Noticias360 - Agregador Inteligente de Noticias

Sistema automatizado que:
- âœ… Extrae noticias de 8 medios argentinos (RSS)
- âœ… Clasifica automÃ¡ticamente por categorÃ­as
- âœ… Genera resÃºmenes con IA (Gemini)
- âœ… Detecta temas relevantes y su evoluciÃ³n temporal
- âœ… Extrae contenido completo de artÃ­culos (web scraping)

## ğŸ› ï¸ Stack

**Backend:** Python 3.11+ â€¢ feedparser â€¢ google-genai â€¢ newspaper3k â€¢ trafilatura  
**Frontend:** HTML5 â€¢ Tailwind CSS â€¢ JavaScript  
**Deploy:** Vercel + GitHub Actions

## ğŸ“ Estructura Simplificada

```
â”œâ”€â”€ data/                               # Backend (ignorado en Git)
â”‚   â”œâ”€â”€ raw/                            # Noticias RSS crudas
â”‚   â”œâ”€â”€ normalized/                     # Fechas normalizadas
â”‚   â”œâ”€â”€ noticias_*.json                 # Consolidado diario
â”‚   â”œâ”€â”€ noticias_contenido_*.json       # Con contenido completo (scraping)
â”‚   â””â”€â”€ temas/
â”‚       â”œâ”€â”€ temas_*.json                # Temas detectados por dÃ­a
â”‚       â””â”€â”€ historico_temas.json        # EvoluciÃ³n temporal de temas
â”‚
â”œâ”€â”€ frontend/data/                      # Para el sitio web (EN Git)
â”‚   â”œâ”€â”€ noticias_YYYY-MM-DD.json        # Noticias del dÃ­a (por fecha)
â”‚   â”œâ”€â”€ resumenes_YYYY-MM-DD.json       # ResÃºmenes por categorÃ­a (por fecha)
â”‚   â””â”€â”€ temas_latest.json               # Temas del dÃ­a (se sobrescribe)
â”‚
â”œâ”€â”€ scripts/                            # 7 scripts del pipeline
â”‚   â”œâ”€â”€ ejecutar_pipeline.py            # â† EJECUTAR ESTE
â”‚   â”œâ”€â”€ extraer_feeds.py
â”‚   â”œâ”€â”€ normalizar_fechas.py
â”‚   â”œâ”€â”€ integrar_fuentes.py
â”‚   â”œâ”€â”€ clasificar_categorias_url.py
â”‚   â”œâ”€â”€ extraer_contenido.py            # Scraping (nuevo)
â”‚   â”œâ”€â”€ generar_resumenes_gemini.py
â”‚   â””â”€â”€ agrupar_temas.py                # DetecciÃ³n de temas (nuevo)
â”‚
â””â”€â”€ feeds_config.json                   # ConfiguraciÃ³n RSS
```

## âš™ï¸ InstalaciÃ³n RÃ¡pida

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Configurar API de Gemini (crear archivo .env)
echo "GEMINI_API_KEY=tu-api-key" > .env

# 3. Ejecutar pipeline completo
python scripts/ejecutar_pipeline.py

# 4. Ver en navegador
python server.py
# Abre: http://localhost:8000
```

**Obtener API key gratuita:** https://aistudio.google.com/apikey

## ğŸ”„ Pipeline Completo (7 Pasos)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python scripts/ejecutar_pipeline.py                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 1: extraer_feeds.py (~30s)
  â€¢ Descarga RSS de 8 fuentes (ClarÃ­n, La NaciÃ³n, Infobae, etc.)
  â€¢ Guarda: data/raw/*.json

PASO 2: normalizar_fechas.py (~5s)
  â€¢ Convierte fechas a UTC-3 (Argentina)
  â€¢ Calcula horas_atras
  â€¢ Guarda: data/normalized/*.json

PASO 3: integrar_fuentes.py (~5s)
  â€¢ Consolida todas las fuentes en un archivo
  â€¢ Elimina duplicados
  â€¢ Guarda: data/noticias_2025-11-12.json

PASO 4: clasificar_categorias_url.py (~5s)
  â€¢ Clasifica por URL (Internacional, PolÃ­tica, EconomÃ­a, etc.)
  â€¢ Limpia frontend/data/ (solo noticias/resÃºmenes viejos, NO temas)
  â€¢ Copia: frontend/data/noticias_2025-11-12.json â† FRONTEND

PASO 5: extraer_contenido.py (~4-5 min) [OPCIONAL]
  â€¢ Web scraping de 150 noticias (Internacional, PolÃ­tica, EconomÃ­a)
  â€¢ Usa Newspaper3k + Trafilatura
  â€¢ Guarda: data/noticias_contenido_latest.json â† PARA IA

PASO 6: generar_resumenes_gemini.py (~30s)
  â€¢ Genera resÃºmenes de 4 categorÃ­as con Gemini
  â€¢ Guarda: data/resumenes_2025-11-12.json
  â€¢ Copia: frontend/data/resumenes_2025-11-12.json â† FRONTEND

PASO 7: agrupar_temas.py (~2-3 min)
  â€¢ Detecta 10 temas relevantes con Gemini
  â€¢ Identifica NUEVOS vs RECURRENTES (histÃ³rico)
  â€¢ Integra resÃºmenes para temas recurrentes
  â€¢ Calcula tendencias y mÃ©tricas
  â€¢ Guarda:
    â”œâ”€ data/temas/historico_temas.json (tracking interno)
    â”œâ”€ data/temas/temas_2025-11-12.json (backup)
    â””â”€ frontend/data/temas_latest.json â† FRONTEND

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TIEMPO TOTAL: ~8-12 minutos (con scraping)
              ~4-6 minutos (sin scraping - comentar PASO 5)
```

## ğŸ“‹ Archivos que DEBE tener `frontend/data/`

**Solo 3 archivos** (el frontend busca por fecha):

```
frontend/data/
â”œâ”€â”€ noticias_2025-11-12.json      # Noticias del dÃ­a
â”œâ”€â”€ resumenes_2025-11-12.json     # ResÃºmenes por categorÃ­a
â””â”€â”€ temas_latest.json             # Temas del dÃ­a
```

**CÃ³mo los busca el frontend:**
- `noticias_*.json`: Calcula fecha actual (UTC-3 Argentina) â†’ busca `noticias_YYYY-MM-DD.json`
- `resumenes_*.json`: Calcula fecha actual â†’ busca `resumenes_YYYY-MM-DD.json`
- `temas_latest.json`: Siempre busca este nombre fijo

**Nota:** El histÃ³rico de temas (`historico_temas.json`) solo se guarda en `data/temas/` para tracking interno, el frontend no lo usa.

---

## ğŸ¯ Scripts Individuales (para debugging)

```bash
# Extraer contenido completo (solo si necesitas regenerarlo)
python scripts/extraer_contenido.py

# Detectar temas (solo si fallÃ³ en el pipeline)
python scripts/agrupar_temas.py

# Probar conexiÃ³n con Gemini
python scripts/test_gemini_api.py
```

## ğŸ§  Sistema de Temas (Fase 2 - Con HistÃ³rico)

### **QuÃ© hace:**
- Detecta **10 temas relevantes** por dÃ­a usando IA
- Identifica si son **NUEVOS** â­ o **RECURRENTES** ğŸ”„
- Para temas recurrentes: **integra** el resumen anterior + noticias nuevas
- Calcula **tendencias** (creciente â†‘, estable â†’, decreciente â†“)
- Mantiene **histÃ³rico completo** de evoluciÃ³n dÃ­a a dÃ­a

### **Ejemplo de evoluciÃ³n:**

**DÃ­a 1 (10/11):** 
- "Cotizaciones del DÃ³lar" â­ NUEVO
- Resumen generado desde cero

**DÃ­a 2 (11/11):**
- "Cotizaciones del DÃ³lar" ğŸ”„ 2 dÃ­as, â†‘ CRECIENTE
- Resumen integrado (mantiene contexto + agrega novedades)

**DÃ­a 5 (14/11):**
- "Cotizaciones del DÃ³lar" ğŸ”¥ 5 dÃ­as, â†’ ESTABLE
- Si no aparece por 3+ dÃ­as â†’ marca como "inactivo"

### **Archivos generados:**
- `frontend/data/temas_latest.json`: Temas de HOY para el frontend (10 temas con resÃºmenes)
- `data/temas/historico_temas.json`: Tracking interno con todas las apariciones (NO se copia a frontend)

---

## ğŸ¨ Frontend

**Vista Noticias:**
- NavegaciÃ³n: Internacional, PolÃ­tica, EconomÃ­a, Sociedad
- Resumen colapsable por categorÃ­a (IA)
- Tarjetas intercaladas por fuente

**Vista Temas:**
- Filtrado por categorÃ­a (igual que noticias)
- Badges: â­ NUEVO, â†‘ EN ALZA, ğŸ”¥ X dÃ­as
- Resumen completo expandible

---

## âš ï¸ SoluciÃ³n de Problemas

### Error 503 en Gemini (servicio sobrecargado)
```bash
# Esperar 1-2 minutos y reintentar solo el paso que fallÃ³:
python scripts/generar_resumenes_gemini.py  # Si fallÃ³ PASO 6
python scripts/agrupar_temas.py             # Si fallÃ³ PASO 7
```

### Falta `temas_latest.json` en frontend/
```bash
# Ejecutar solo detecciÃ³n de temas:
python scripts/agrupar_temas.py
```

### Acelerar el pipeline
Comentar PASO 5 en `ejecutar_pipeline.py` (lÃ­neas 97-109) para saltear el scraping.
Los temas funcionarÃ¡n igual pero con menos detalle.

---

## ğŸ“Š Fuentes Configuradas

ClarÃ­n â€¢ La NaciÃ³n â€¢ Infobae â€¢ PÃ¡gina 12 â€¢ Ãmbito â€¢ Perfil â€¢ Minuto1 â€¢ iProfesional

EditÃ¡ `feeds_config.json` para agregar mÃ¡s.

---

## ğŸ¤– GitHub Actions (AutomatizaciÃ³n)

El workflow `.github/workflows/update_news.yml` ejecuta el pipeline **cada 3 horas automÃ¡ticamente**.

### **ConfiguraciÃ³n (si aÃºn no lo hiciste):**

1. **Configurar GEMINI_API_KEY en GitHub Secrets**
   - Ir a: `Settings` â†’ `Secrets and variables` â†’ `Actions`
   - Click en `New repository secret`
   - Name: `GEMINI_API_KEY`
   - Secret: Tu API key de Gemini
   - Click en `Add secret`

2. **Subir cambios al repositorio**
   ```bash
   git add .
   git commit -m "Update: nuevos scripts y workflow mejorado"
   git push
   ```

3. **Probar el workflow manualmente**
   - Ir a: `Actions` â†’ `Actualizar noticias cada 3h`
   - Click en `Run workflow` â†’ `Run workflow`
   - Esperar 8-12 minutos y revisar logs

### **QuÃ© hace el workflow:**
- âœ… Ejecuta `ejecutar_pipeline.py` completo
- âœ… Limpia archivos antiguos de `frontend/data/`
- âœ… Commitea solo archivos del dÃ­a actual
- âœ… Push automÃ¡tico a `main` con `[ci skip]` para evitar loops

### **Archivos que se commitean:**
```
frontend/data/
â”œâ”€â”€ noticias_2025-11-12.json    (se sobrescribe cada 3h)
â”œâ”€â”€ resumenes_2025-11-12.json   (se sobrescribe cada 3h)
â””â”€â”€ temas_latest.json           (se sobrescribe cada 3h)
```

---

**Ãšltima actualizaciÃ³n:** Noviembre 2025

