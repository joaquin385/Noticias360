# ğŸ“° Noticias360 - Agregador de Noticias RSS

Sistema automatizado que:
- âœ… Extrae noticias de 8 medios argentinos (RSS)
- âœ… Normaliza fechas y consolida noticias en un Ãºnico JSON diario
- âœ… Clasifica automÃ¡ticamente por categorÃ­as (heurÃ­sticas por URL / RSS)

## ğŸ› ï¸ Stack

**Backend:** Python 3.11+ â€¢ feedparser â€¢ python-dateutil  
**Frontend:** HTML5 â€¢ Tailwind CSS â€¢ JavaScript  
**Deploy:** Vercel + GitHub Actions

## ğŸ“ Estructura Simplificada

```
â”œâ”€â”€ data/                               # Backend (ignorado en Git)
â”‚   â”œâ”€â”€ raw/                            # Noticias RSS crudas
â”‚   â”œâ”€â”€ normalized/                     # Fechas normalizadas
â”‚   â”œâ”€â”€ noticias_*.json                 # Consolidado diario
â”‚   â”œâ”€â”€ noticias_contenido_*.json       # Con contenido completo (scraping - opcional/legacy)
â”‚   â””â”€â”€ temas/                          # Datos de temas IA (legacy, opcional)
â”‚       â”œâ”€â”€ temas_*.json                # Temas detectados por dÃ­a
â”‚       â””â”€â”€ historico_temas.json        # EvoluciÃ³n temporal de temas
â”‚
â”œâ”€â”€ frontend/data/                      # Para el sitio web (EN Git)
â”‚   â””â”€â”€ noticias_YYYY-MM-DD.json        # Noticias del dÃ­a (por fecha)
â”‚
â”œâ”€â”€ scripts/                            # 4 pasos principales (+ scripts legacy IA)
â”‚   â”œâ”€â”€ ejecutar_pipeline.py            # â† EJECUTAR ESTE (modo sin IA)
â”‚   â”œâ”€â”€ extraer_feeds.py
â”‚   â”œâ”€â”€ normalizar_fechas.py
â”‚   â”œâ”€â”€ integrar_fuentes.py
â”‚   â”œâ”€â”€ clasificar_categorias_url.py
â”‚   â”œâ”€â”€ extraer_contenido.py            # Scraping (opcional / legacy IA)
â”‚   â”œâ”€â”€ generar_resumenes_gemini.py     # ResÃºmenes con IA (legacy, desactivado del pipeline)
â”‚   â””â”€â”€ agrupar_temas.py                # Temas con IA (legacy, desactivado del pipeline)
â”‚
â””â”€â”€ feeds_config.json                   # ConfiguraciÃ³n RSS
```

## âš™ï¸ InstalaciÃ³n RÃ¡pida (modo sin IA)

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Ejecutar pipeline (modo sin IA)
python scripts/ejecutar_pipeline.py

# 3. Ver en navegador
python server.py
# Abre: http://localhost:8000
```

## ğŸ”„ Pipeline Completo (4 Pasos activos)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python scripts/ejecutar_pipeline.py      # Modo sin IA         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PASO 1: extraer_feeds.py (~30s)
  â€¢ Descarga RSS de 8 fuentes (ClarÃ­n, La NaciÃ³n, Infobae, etc.)
  â€¢ Guarda: data/raw/*.json

PASO 2: normalizar_fechas.py (~5s)
  â€¢ Convierte fechas a UTC-3 (Argentina)
  â€¢ Calcula horas_atras
  â€¢ Guarda: data/normalized/*.json

PASO 3: integrar_fuentes.py (~5s)
  â€¢ Consolida todas las fuentes en un archivo
  â€¢ Elimina duplicados
  â€¢ Guarda: data/noticias_YYYY-MM-DD.json

PASO 4: clasificar_categorias_url.py (~5s)
  â€¢ Clasifica por URL (Internacional, PolÃ­tica, EconomÃ­a, etc.)
  â€¢ Limpia frontend/data/ (noticias viejas)
  â€¢ Copia: frontend/data/noticias_YYYY-MM-DD.json â† FRONTEND

PASOS 5-7 (scraping + IA) estÃ¡n desactivados en el pipeline actual.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TIEMPO TOTAL: ~4-6 minutos (modo sin IA)
```

## ğŸ“‹ Archivos que DEBE tener `frontend/data/`

**Archivo principal que usa el frontend:**

```
frontend/data/
â””â”€â”€ noticias_YYYY-MM-DD.json      # Noticias del dÃ­a
```

**CÃ³mo lo busca el frontend:**
- Calcula fecha actual (UTC-3 Argentina) â†’ busca `noticias_YYYY-MM-DD.json`

---

## ğŸ¯ Scripts Individuales (para debugging)

```bash
# Extraer contenido completo (solo si necesitas regenerarlo)
python scripts/extraer_contenido.py

# Detectar temas (solo si fallÃ³ en el pipeline)
python scripts/agrupar_temas.py

# Probar conexiÃ³n con Gemini
python scripts/test_gemini_api.py
```

## ğŸ¨ Frontend

**Vista Noticias:**
- NavegaciÃ³n: Internacional, PolÃ­tica, EconomÃ­a, Sociedad
- Tarjetas intercaladas por fuente

---

## âš ï¸ SoluciÃ³n de Problemas

### Acelerar el pipeline
En modo sin IA no es necesario configurar API keys ni scraping extra; el pipeline ya corre con los 4 pasos bÃ¡sicos.

---

## ğŸ“Š Fuentes Configuradas

ClarÃ­n â€¢ La NaciÃ³n â€¢ Infobae â€¢ PÃ¡gina 12 â€¢ Ãmbito â€¢ Perfil â€¢ Minuto1 â€¢ iProfesional

EditÃ¡ `feeds_config.json` para agregar mÃ¡s.

---

## ğŸ¤– GitHub Actions (AutomatizaciÃ³n, modo sin IA)

El workflow `.github/workflows/update_news.yml` ejecuta el pipeline **cada 3 horas automÃ¡ticamente** en modo sin IA.

### **ConfiguraciÃ³n (si aÃºn no lo hiciste):**

1. **Subir cambios al repositorio**
   ```bash
   git add .
   git commit -m "Update: nuevos scripts y workflow mejorado"
   git push
   ```

2. **Probar el workflow manualmente**
   - Ir a: `Actions` â†’ `Actualizar noticias cada 3h`
   - Click en `Run workflow` â†’ `Run workflow`
   - Esperar 8-12 minutos y revisar logs

### **QuÃ© hace el workflow:**
- âœ… Ejecuta `ejecutar_pipeline.py` (sin IA)
- âœ… Limpia archivos antiguos de `frontend/data/`
- âœ… Commitea solo archivos del dÃ­a actual (`noticias_YYYY-MM-DD.json`)
- âœ… Push automÃ¡tico a `main` con `[ci skip]` para evitar loops

---

**Ãšltima actualizaciÃ³n:** Noviembre 2025

